{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP for Guitar: Google Colab チュートリアル\n",
    "\n",
    "DDSP for Guitar チュートリアルへようこそ！このノートブックでは、環境のセットアップ、ダミーデータセットの作成、モデルのトレーニング、そして音声の合成までの一連の流れをご案内します。\n",
    "\n",
    "## 概要\n",
    "\n",
    "このプロジェクトは、エレキギターの音声を合成するための微分可能デジタル信号処理（DDSP）の実装です。以下の機能を備えています:\n",
    "\n",
    "- ハーモニック合成（倍音生成）\n",
    "- ノイズ合成\n",
    "- ウェーブシェイパー（非線形歪み）\n",
    "- トーンスタック（3バンドEQ）\n",
    "- トランジェント分離\n",
    "\n",
    "## 必要な環境\n",
    "\n",
    "- Google Colab (GPU推奨)\n",
    "- Python 3.8以上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境のセットアップ\n",
    "\n",
    "まず、リポジトリをクローンして必要な依存関係をインストールします。\n",
    "\n",
    "**重要:** 次のセルで `<YOUR_REPOSITORY_URL>` を実際のリポジトリのURLに置き換えてください（例: `https://github.com/username/ddsp_guitar.git`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リポジトリをクローン\n",
    "!git clone <YOUR_REPOSITORY_URL>\n",
    "%cd ddsp_guitar\n",
    "\n",
    "# 依存関係をインストール\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# GPU使用可否を確認\n",
    "import torch\n",
    "print(f\"PyTorch バージョン: {torch.__version__}\")\n",
    "print(f\"CUDA 利用可能: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU デバイス: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ダミーデータセットの作成\n",
    "\n",
    "トレーニングに使用するダミーデータセットを作成します。実際の使用では、エレキギターのDI（Direct Input）音源を用意してください。\n",
    "\n",
    "ここでは、サイン波をベースにした合成音声を生成してデータセットとして使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダミーデータセットを作成\n",
    "!python scripts/create_dummy_dataset.py \\\n",
    "    --output_dir dataset/train/audio \\\n",
    "    --num_samples 50 \\\n",
    "    --sample_rate 48000 \\\n",
    "    --duration 2.0\n",
    "\n",
    "# 検証用データも作成\n",
    "!python scripts/create_dummy_dataset.py \\\n",
    "    --output_dir dataset/val/audio \\\n",
    "    --num_samples 10 \\\n",
    "    --sample_rate 48000 \\\n",
    "    --duration 2.0\n",
    "\n",
    "print(\"\\nデータセットの作成が完了しました！\")\n",
    "print(\"トレーニングデータ: dataset/train/audio/\")\n",
    "print(\"検証データ: dataset/val/audio/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルのトレーニング\n",
    "\n",
    "データセットが準備できたので、モデルをトレーニングします。\n",
    "\n",
    "**注意:** 完全なトレーニングには時間がかかるため、ここでは少ないエポック数で実行します。実際のプロジェクトでは、より多くのエポック（100エポック以上）を推奨します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングを実行（デモ用に10エポック）\n",
    "!python scripts/train.py \\\n",
    "    --train_dir dataset/train/audio \\\n",
    "    --val_dir dataset/val/audio \\\n",
    "    --sample_rate 48000 \\\n",
    "    --segment_seconds 1.0 \\\n",
    "    --batch_size 4 \\\n",
    "    --epochs 10 \\\n",
    "    --lr 1e-4 \\\n",
    "    --num_harm 64 \\\n",
    "    --num_noise 8 \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --save_every 5 \\\n",
    "    --device cuda\n",
    "\n",
    "print(\"\\nトレーニングが完了しました！\")\n",
    "print(\"チェックポイント: checkpoints/\")\n",
    "print(\"TensorBoardログ: logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トレーニング結果の確認\n",
    "\n",
    "TensorBoardを使用してトレーニングの進捗を確認します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoardを起動\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "# 最新のログディレクトリを表示\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir = Path('logs')\n",
    "if log_dir.exists():\n",
    "    log_dirs = sorted(log_dir.glob('*'))\n",
    "    if log_dirs:\n",
    "        print(f\"最新のログ: {log_dirs[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. モデルの推論\n",
    "\n",
    "トレーニングしたモデルを使って音声を合成してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ddsp_guitar.model import GuitarDDSP\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "\n",
    "# 設定\n",
    "SAMPLE_RATE = 48000\n",
    "AUDIO_LENGTH_SECONDS = 2\n",
    "FRAME_RATE = 250\n",
    "HOP_SIZE = SAMPLE_RATE // FRAME_RATE\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# モデルの読み込み\n",
    "model = GuitarDDSP(sample_rate=SAMPLE_RATE, num_harm=64, num_noise=8, device=device)\n",
    "\n",
    "# チェックポイントから重みを読み込み（ベストモデルまたは最終モデル）\n",
    "checkpoint_path = Path('checkpoints/best_model.pt')\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path = Path('checkpoints/final_model.pt')\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"チェックポイントを読み込みました: {checkpoint_path}\")\n",
    "else:\n",
    "    print(\"チェックポイントが見つかりません。初期化されたモデルを使用します。\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"モデルパラメータ数: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 テスト音声の合成\n",
    "\n",
    "ダミーの入力パラメータを使って音声を合成します。実際の使用では、入力音声から特徴量（基本周波数、ラウドネスなど）を抽出します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力パラメータの作成\n",
    "batch_size = 1\n",
    "audio_length_samples = AUDIO_LENGTH_SECONDS * SAMPLE_RATE\n",
    "n_frames = audio_length_samples // HOP_SIZE\n",
    "\n",
    "# 条件付け入力（通常は入力音声から抽出される特徴量）\n",
    "conditioning_input = torch.randn(batch_size, n_frames).to(device)\n",
    "\n",
    "# 基本周波数（f0）- A2音（110Hz）からE3音（165Hz）へのスライド\n",
    "f0_start = 110.0  # A2\n",
    "f0_end = 165.0    # E3\n",
    "f0_values = torch.linspace(f0_start, f0_end, n_frames).unsqueeze(0).to(device)\n",
    "\n",
    "# ラウドネス - 時間とともに減衰\n",
    "loudness_values = torch.linspace(-20.0, -40.0, n_frames).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"入力形状:\")\n",
    "print(f\"  conditioning_input: {conditioning_input.shape}\")\n",
    "print(f\"  f0: {f0_values.shape}\")\n",
    "print(f\"  loudness: {loudness_values.shape}\")\n",
    "\n",
    "# 音声を合成\n",
    "print(\"\\n音声を合成中...\")\n",
    "with torch.no_grad():\n",
    "    output_audio = model(conditioning_input, f0_values, loudness_values)\n",
    "\n",
    "print(\"合成完了！\")\n",
    "print(f\"出力音声形状: {output_audio.shape}\")\n",
    "\n",
    "# CPUに移動して保存\n",
    "output_audio_np = output_audio.cpu().numpy().squeeze()\n",
    "\n",
    "# 正規化\n",
    "output_audio_np = output_audio_np / (np.abs(output_audio_np).max() + 1e-8) * 0.9\n",
    "\n",
    "# 保存\n",
    "output_filename = 'synthesized_guitar.wav'\n",
    "sf.write(output_filename, output_audio_np, SAMPLE_RATE)\n",
    "print(f\"音声を保存しました: {output_filename}\")\n",
    "\n",
    "# 再生\n",
    "Audio(output_audio_np, rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 メロディの合成\n",
    "\n",
    "複数の音符を使ってシンプルなメロディを合成してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メロディの定義（音符と長さ）\n",
    "# C4, D4, E4, F4, G4, A4, B4, C5（ドレミファソラシド）\n",
    "melody_notes = [\n",
    "    (261.63, 0.3),  # C4\n",
    "    (293.66, 0.3),  # D4\n",
    "    (329.63, 0.3),  # E4\n",
    "    (349.23, 0.3),  # F4\n",
    "    (392.00, 0.3),  # G4\n",
    "    (440.00, 0.3),  # A4\n",
    "    (493.88, 0.3),  # B4\n",
    "    (523.25, 0.5),  # C5（最後の音は長く）\n",
    "]\n",
    "\n",
    "# メロディ全体の長さを計算\n",
    "total_duration = sum(duration for _, duration in melody_notes)\n",
    "total_samples = int(total_duration * SAMPLE_RATE)\n",
    "total_frames = total_samples // HOP_SIZE\n",
    "\n",
    "# f0とラウドネスのシーケンスを作成\n",
    "f0_sequence = []\n",
    "loudness_sequence = []\n",
    "\n",
    "current_frame = 0\n",
    "for freq, duration in melody_notes:\n",
    "    num_frames = int((duration * SAMPLE_RATE) // HOP_SIZE)\n",
    "    \n",
    "    # 各音符のf0（一定）\n",
    "    f0_sequence.extend([freq] * num_frames)\n",
    "    \n",
    "    # ラウドネス（アタック→サスティン→リリース）\n",
    "    attack_frames = num_frames // 4\n",
    "    sustain_frames = num_frames // 2\n",
    "    release_frames = num_frames - attack_frames - sustain_frames\n",
    "    \n",
    "    attack = np.linspace(-35, -15, attack_frames)\n",
    "    sustain = np.full(sustain_frames, -15)\n",
    "    release = np.linspace(-15, -45, release_frames)\n",
    "    \n",
    "    loudness_sequence.extend(attack.tolist())\n",
    "    loudness_sequence.extend(sustain.tolist())\n",
    "    loudness_sequence.extend(release.tolist())\n",
    "\n",
    "# テンソルに変換\n",
    "f0_tensor = torch.tensor(f0_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "loudness_tensor = torch.tensor(loudness_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "conditioning_tensor = torch.randn(1, len(f0_sequence)).to(device)\n",
    "\n",
    "print(f\"メロディの長さ: {total_duration:.2f}秒\")\n",
    "print(f\"フレーム数: {len(f0_sequence)}\")\n",
    "\n",
    "# メロディを合成\n",
    "print(\"\\nメロディを合成中...\")\n",
    "with torch.no_grad():\n",
    "    melody_audio = model(conditioning_tensor, f0_tensor, loudness_tensor)\n",
    "\n",
    "melody_audio_np = melody_audio.cpu().numpy().squeeze()\n",
    "melody_audio_np = melody_audio_np / (np.abs(melody_audio_np).max() + 1e-8) * 0.9\n",
    "\n",
    "# 保存\n",
    "melody_filename = 'synthesized_melody.wav'\n",
    "sf.write(melody_filename, melody_audio_np, SAMPLE_RATE)\n",
    "print(f\"メロディを保存しました: {melody_filename}\")\n",
    "\n",
    "# 再生\n",
    "Audio(melody_audio_np, rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 次のステップ\n",
    "\n",
    "このチュートリアルでは基本的な使い方を紹介しましたが、より高度な使い方には以下があります：\n",
    "\n",
    "### 6.1 実際のギター音源を使う\n",
    "\n",
    "ダミーデータの代わりに、実際のエレキギターのDI（Direct Input）音源を使用してトレーニングすることで、より高品質な結果が得られます。\n",
    "\n",
    "```python\n",
    "# 実際のデータセットの準備\n",
    "# 1. wavファイルをdataset/train/audio/に配置\n",
    "# 2. 同様にdataset/val/audio/に検証データを配置\n",
    "# 3. トレーニングスクリプトを実行\n",
    "```\n",
    "\n",
    "### 6.2 ハイパーパラメータの調整\n",
    "\n",
    "以下のパラメータを調整することで、モデルの性能を向上させることができます：\n",
    "\n",
    "- `num_harm`: ハーモニクス数（デフォルト: 64）\n",
    "- `num_noise`: ノイズバンド数（デフォルト: 8）\n",
    "- `lr`: 学習率（デフォルト: 1e-4）\n",
    "- `batch_size`: バッチサイズ（GPUメモリに応じて調整）\n",
    "- `epochs`: エポック数（100エポック以上推奨）\n",
    "\n",
    "### 6.3 リアルタイム処理\n",
    "\n",
    "`ddsp_guitar/rt/engine.py`を使用することで、リアルタイムでの音声処理が可能です。\n",
    "\n",
    "### 6.4 モデルの保存とエクスポート\n",
    "\n",
    "```python\n",
    "# モデルの保存\n",
    "torch.save(model.state_dict(), 'guitar_ddsp_model.pth')\n",
    "\n",
    "# モデルの読み込み\n",
    "model.load_state_dict(torch.load('guitar_ddsp_model.pth'))\n",
    "```\n",
    "\n",
    "## 参考資料\n",
    "\n",
    "- [DDSP論文](https://arxiv.org/abs/2001.04643)\n",
    "- [プロジェクトリポジトリ](https://github.com/your-repo/ddsp_guitar)\n",
    "- [ドキュメント](docs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. トラブルシューティング\n",
    "\n",
    "### GPU メモリ不足の場合\n",
    "\n",
    "バッチサイズを減らすか、セグメント長を短くしてください：\n",
    "\n",
    "```bash\n",
    "!python scripts/train.py --batch_size 2 --segment_seconds 0.5\n",
    "```\n",
    "\n",
    "### トレーニングが不安定な場合\n",
    "\n",
    "学習率を下げるか、勾配クリッピングを調整してください：\n",
    "\n",
    "```bash\n",
    "!python scripts/train.py --lr 5e-5 --grad_clip 0.5\n",
    "```\n",
    "\n",
    "### 音質が悪い場合\n",
    "\n",
    "- より多くのデータでトレーニングする\n",
    "- エポック数を増やす（100エポック以上）\n",
    "- ハーモニクス数を増やす（`--num_harm 128`）\n",
    "- 実際のギター音源を使用する\n",
    "\n",
    "## おわりに\n",
    "\n",
    "このチュートリアルを通じて、DDSP for Guitarの基本的な使い方を学びました。\n",
    "質問や問題がある場合は、GitHubのIssueで報告してください。\n",
    "\n",
    "Happy coding!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
