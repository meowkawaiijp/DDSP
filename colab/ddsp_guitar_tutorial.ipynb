{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP for Guitar: Google Colab Tutorial\n",
    "\n",
    "Welcome to the DDSP for Guitar tutorial! This notebook will guide you through the process of setting up the environment, loading the model, and synthesizing your first guitar sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, we need to clone the repository and install the required dependencies. \n",
    "\n",
    "**Important:** In the next cell, please replace `<YOUR_REPOSITORY_URL>` with the actual URL of your forked or cloned repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone <YOUR_REPOSITORY_URL>\n",
    "%cd ddsp_guitar\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model\n",
    "\n",
    "Now that the environment is set up, let's import the necessary libraries and load the `GuitarDDSP` model. The model will automatically be placed on the GPU if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ddsp_guitar.model import GuitarDDSP\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "\n",
    "# --- Configuration ---\n",
    "SAMPLE_RATE = 48000\n",
    "AUDIO_LENGTH_SECONDS = 4\n",
    "FRAME_RATE = 250\n",
    "HOP_SIZE = SAMPLE_RATE // FRAME_RATE\n",
    "\n",
    "# --- Load Model ---\n",
    "# The model will automatically use the GPU if available.\n",
    "model = GuitarDDSP(sample_rate=SAMPLE_RATE)\n",
    "print(f\"Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthesize Audio\n",
    "\n",
    "With the model loaded, we can now synthesize some audio. We'll create some dummy input tensors for fundamental frequency (f0) and loudness. In a real application, these would be extracted from an audio input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create dummy inputs ---\n",
    "batch_size = 1\n",
    "audio_length_samples = AUDIO_LENGTH_SECONDS * SAMPLE_RATE\n",
    "n_frames = audio_length_samples // HOP_SIZE\n",
    "\n",
    "# Dummy conditioning input (in a real scenario, this would be derived from audio features)\n",
    "conditioning_input = torch.randn(batch_size, n_frames)\n",
    "\n",
    "# Dummy f0 and loudness\n",
    "dummy_f0_hz = torch.full((batch_size, n_frames), 110.0) # A2 note\n",
    "dummy_loudness = torch.full((batch_size, n_frames), -30.0)\n",
    "\n",
    "# --- Synthesize Audio ---\n",
    "print(\"Synthesizing audio...\")\n",
    "with torch.no_grad():\n",
    "    output_audio = model(conditioning_input, dummy_f0_hz, dummy_loudness)\n",
    "print(\"Synthesis complete!\")\n",
    "\n",
    "# --- Post-processing ---\n",
    "output_audio_np = output_audio.cpu().numpy().squeeze()\n",
    "\n",
    "# --- Save and Listen ---\n",
    "output_filename = 'output_audio.wav'\n",
    "sf.write(output_filename, output_audio_np, SAMPLE_RATE)\n",
    "\n",
    "print(f\"Audio saved to {output_filename}\")\n",
    "Audio(output_audio_np, rate=SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
